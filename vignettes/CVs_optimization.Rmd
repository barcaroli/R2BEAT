---
title: "R2BEAT \n Optimization of precision constraints" 
date: "`r Sys.Date()` <br>  <br> <br>"
output: 
  bookdown::html_document2:
    df_print: kable
    highlight: tango
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
      smooth_scroll: yes
    toc_title: 
    # code_folding: hide
keep_md: TRUE
#bibliography: 
#link-citations: yes
#linkcolor: red
vignette: >
  %\VignetteIndexEntry{R2BEAT two-stage sampling design workflow starting from a previous survey}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# devtools::install_github("barcaroli/R2BEAT")
# devtools::install_github("barcaroli/QGA")
library(R2BEAT)
if (!require("kableExtra", character.only = TRUE)) {
  install.packages("kableExtra")
}
library(kableExtra)
if (!require("QGA", character.only = TRUE)) {
  devtools::install_github("barcaroli/QGA")
}
library("QGA", character.only = TRUE)
```

# Introduction

1. In a multivariate stratified sampling design procedure, very often the values of the precision constraints (expressed in terms of maximum expected coefficients of variation set on the target estimates) are set taking into account the budget constraints that oblige to a given affordable sample size. This implies that an expert has to fine tune the precision constraints (increasing and decreasing them) until a satisfactory set is determined. When the number of variables and/or domains is not small, this job can be very expensive. To handle this issue, a new function, the "pareto" one, has been developed, in order to determine an optimal solution in terms of precision constraints, given the affordable sample size. 

2. Another issue is related to the characteristics of the allocation of sampling units in the strata: it may happen that it can be very different from the proportional one. In some cases it may be desirable to limit the distance between the two allocations, avoiding the cases of too much oversampling in small strata. 
It is possible to obtain this, also in this case by changing the precision constraints.
To get this result, it is possible to make use of a genetic algorithm, that explores the space of the possible combinations of values of the precision constraints, with the aim of maximizing the coefficient of correlation of the two distributions (optimal and proportional allocations).

# Setting

In this study, we assume that the affordable sample size (given the budget) is set to 50,000.
We show how it is possible to start from a "neutral" set of precision constraints (equal CVs for all variables for a given domain level), and modify them, so to:

* first, balance the influence of all the variables on the determination of the best allocation, or at least, to avoid that only a small fraction plays a role;

* secondly, get the optimal allocation closer to the proportional one.

We consider a strata dataframe containing information on labor force in a country. Each stratum belongs to three different domain levels:

```{r message=FALSE, warning=FALSE}
load("strata.RData")
# strata$DOM3 <- NULL  # de-comment if you want to execute on 2 domains
str(strata)
```

We consider as target variables:

```{r message=FALSE, warning=FALSE}
target_vars <- c("active","inactive","unemployed","income_hh")
target_vars
```

We initially set the following precision constraints:

```{r message=FALSE, warning=FALSE}
# De-comment if you want to execute on 2 domains
# cv_equal <- as.data.frame(list(DOM = c("DOM1","DOM2"),
#                          CV1 = c(0.05,0.05),
#                          CV2 = c(0.05,0.05),
#                          CV3 = c(0.05,0.05),
#                          CV4 = c(0.05,0.05)))
# De-comment if you want to execute on 3 domains
cv_equal <- as.data.frame(list(DOM = c("DOM1","DOM2","DOM3"),
                         CV1 = c(0.05,0.05,0.05),
                         CV2 = c(0.05,0.05,0.05),
                         CV3 = c(0.05,0.05,0.05),
                         CV4 = c(0.05,0.05,0.05)))
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(
  cv_equal,
  digits = 3,
  caption = "Coefficients of variation by domain",
  format = "html"
) |>
  kableExtra::kable_styling(
    bootstrap_options = c("striped", "condensed"),
    full_width = FALSE
  )
```


and proceed with an initial allocation:

```{r message=FALSE, warning=FALSE}
equal <- R2BEAT::beat.1st(strata,cv_equal)
```

with the following total sample size:

```{r message=FALSE, warning=FALSE}
sum(equal$alloc$ALLOC[-nrow(equal$alloc)])
```

# 2. Modification of initial precision constraints to be compliant with the sample size constraint

We now change the initial precision constraints to reach the desired sample size.

To this aim, we make use of the function 'pareto'. This function determines an optimal set of precision constraints, compliant with the constraint in term of sample size.

Prior to the execution, we set some upper limits to the resulting precision constraints:

```{r message=FALSE, warning=FALSE}
caps <- data.frame(
  DOM = c("DOM3"),
  VAR = c("V1","V2","V3","V4"),
  MAX_CV = c(0.10,0.10,0.10,0.10)
)
caps
```

In this way, we indicate that whatever solution of the "pareto" function must be such that in DOM3 no CV can exceed these values.

```{r message=TRUE, warning=FALSE}
out <- pareto(
  strata = strata,
  current_cvs = cv_equal,
  target_size = 50000,
  tolerance = 5,
  cv_caps = caps,
  beat1cv_fun = beat.1cv_2,
  max_iter = 50,
  max_same_iter = 20,
  plot = TRUE,
  plot_dir = "outputs",
  plot_prefix = "domains",
  show_targets = TRUE,     
  show_caps = TRUE      
)
```


Now, the sample size is in line with the budget constraint:

```{r message=FALSE, warning=FALSE}
sum(out$allocation$n)
```
These are the new CVs:

```{r message=FALSE, warning=FALSE}
cv_pareto <- out$expectedCV 
colnames(cv_pareto)[2:ncol(cv_pareto)] <- paste0("CV",c(1:ncol(cv_pareto)))
write.table(cv_pareto,"./outputs/2.pareto_cvs.csv",sep=",",quote=F,row.names=F)
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(
  cv_pareto,
  digits = 3,
  caption = "Pareto coefficients of variation",
  format = "html"
) |>
  kableExtra::kable_styling(
    bootstrap_options = c("striped", "condensed"),
    full_width = FALSE
  )

```

# 3. Get the optimal allocation closer to the proportional one

Consider the last obtained solution:

```{r message=FALSE, warning=FALSE}
pareto_alloc <- beat.1st(strata,cv_pareto)
knitr::kable(
  pareto_alloc$alloc,
  digits = 3,
  caption = "Pareto allocation",
  format = "html"
) |>
  kableExtra::kable_styling(
    bootstrap_options = c("striped", "condensed"),
    full_width = FALSE
  )
```

We can calculate the Pearson correlation coefficient between the optimal and proportional allocations:

```{r message=FALSE, warning=FALSE}
cor(pareto_alloc$alloc$ALLOC[-nrow(pareto_alloc$alloc)],pareto_alloc$alloc$PROP[-nrow(pareto_alloc$alloc)])
```

We want to get the optimal allocation closer to the proportional one. For instance, we want that the correlation coefficient be increased to 0.9.

To obtain this result, we make use of a particular genetic algorithm, the quantum genetic algorithm, implemented in the R package QGA.

## 3.1 Fitness function

First, we define the following fitness function:
 
```{r message=FALSE, warning=FALSE}
fitness_cvs <- function(solution,eval_func_inputs) {
  strata <- eval_func_inputs[[1]]
  vett <- eval_func_inputs[[2]]
  cv <- eval_func_inputs[[3]]
  vettmin <- eval_func_inputs[[4]]
  vettmax <- eval_func_inputs[[5]]
  nvals <- eval_func_inputs[[6]]
  nvars <- ncol(cv) - 1
  cv_corr <- cv
  n <- 0
  for (k in c(1:nrow(cv))) {
    for (m in c(1:nvars)) {
      n <- n+1
      cv_corr[k,m+1] <- seq(from=vettmin[n],
                            to=vettmax[n],
                            by=(vettmax[n]-vettmin[n])/nvals)[solution[n]]
    }
  }
  cv_corr
  a <- R2BEAT::beat.1st(strata,cv_corr)
  # b <- ks.test(a$alloc$ALLOC[-nrow(a$alloc)],a$alloc$PROP[-nrow(a$alloc)])
  c <- cor(a$alloc$ALLOC[-nrow(a$alloc)],a$alloc$PROP[-nrow(a$alloc)])
  #---------- Fitness function ----------------
  fitness <- c
  return(fitness)
}
```

## 3.2 Setting the parameters 

We set the parameters required by the QGA function:

```{r message=FALSE, warning=FALSE}
nvars <- ncol(cv_pareto) - 1
vett <- NULL
for (k in c(1:nrow(cv_pareto))) {
  vett <- c(vett,cv_pareto[k,c(2:(nvars+1))])
}
vett <- unlist(vett)
vettmin <- vett * 0.65
vettmax <- vett * 1.35
Genome = length(vett)
# Genome = nrow(strata)
nvalues_sol = 16384    # 2^14 ==> 14 qubits
eval_func_inputs = list(strata,
                        vett,
                        cv_pareto,
                        vettmin,
                        vettmax,
                        nvalues_sol
                        )
popsize = 10
generation_max = 100
nvalues_sol = nvalues_sol
thetainit = 3.1415926535 * 0.5
thetaend = 3.1415926535 * 0.025
pop_mutation_rate_init = 1/(popsize + 1)
pop_mutation_rate_end = 1/(popsize + 1)
mutation_rate_init = 1/(Genome + 1)
mutation_rate_end = 1/(Genome + 1)
mutation_flag = TRUE
plotting = TRUE
verbose = FALSE
progress = FALSE
eval_fitness = fitness_cvs
eval_func_inputs = eval_func_inputs
stop_limit = 0.95
```

Some explanations regarding the parameters.
The parameters 'vettmin' and 'vettmax' are fundamental to determine the space of possible solutions.

In fact, consider their values:

```{r message=FALSE, warning=FALSE}
vettmin
```

```{r message=FALSE, warning=FALSE}
vettmax
```

The values in vettmin are the minimum values that the CVs can assume, while the values in vettmax are the maximum ones.

The parameter 'nvalues_sol' indicates the number of possible different values considered for each element in the genome: by setting it to 16384, it means that for each precision constraint, 16384 possible values will be considered, the ones obtained by dividing by 16384 the interval between the minimum and the maximum value set for each precision constraint. 

The parameter 'eval_func_inputs' is a list containing all the information required by the fitness function.

Parameters 'thetainit' and 'thetaend' are the initial and final values expressed in degrees for the rotation gate used in the quantum genetic algorithm. The rotation is high at the beginning of the iterations (to explore mores solutions), lower at the end (to refine the best solution found).

The same for parameters from 'pop_mutation_rate_init' to 'pop_mutation_rate_end', that govern the rate of mutation in the solutions.

Very important is the 'stop_limit' parameter: the iterations are stopped when this limit is reached. In our case it is set to 0.5: this means that when the correlation coefficient reaches this limit, the algorithm stops and output the related solution.

## 3.3 Execution of the genetic algorithm

```{r include=FALSE}
set.seed(1234)
solutionQGA <- QGA(popsize,
                   generation_max,
                   nvalues_sol,
                   Genome,
                   thetainit,
                   thetaend,
                   pop_mutation_rate_init,
                   pop_mutation_rate_end,
                   mutation_rate_init,
                   mutation_rate_end,
                   mutation_flag = TRUE,
                   plotting = FALSE,
                   verbose = FALSE,
                   progress = FALSE,
                   eval_fitness,
                   eval_func_inputs,
                   stop_limit)
```


```{r message=FALSE, warning=FALSE}
solutionQGA[[2]]$fitness_best[nrow(solutionQGA[[2]])]
```


```{r message=FALSE, warning=FALSE}
ndoms = nrow(cv_pareto)
par(mfrow=c(1,2))
#plot vett
plot(vett, col="black", type="l",xaxt = "n", ylim=c(min(vettmin), max(vettmax)), xlab="", ylab="", main="Region of variation for CVs") # xaxt = "n" disables the xaxis ticks
# Generate the vector of labels
vector_labels <- paste("DOM", rep(1:ndoms, each = nvars), 
                       "_V", rep(1:nvars, times = ndoms), 
                       sep = "")
# xticks
axis(side=1,at=seq(1,length(vett)),labels=vector_labels, las=2)
# vettmin and vettmax
lines(vettmin, col="red")
lines(vettmax, col="blue")
# add legend
legend("topleft", legend=c("cv_input", "cv_min", "cv_max"), col = c("black", "red", "blue"), lty=1, cex=0.5)
QGA:::plot_Output(solutionQGA[[2]])
```


We decode the QGA solution:

```{r message=FALSE, warning=FALSE}
solution <- solutionQGA[[1]]
cv_ga <- cv_pareto
n <- 0
nvals <- nvalues_sol
for (k in c(1:nrow(cv_pareto))) {
  for (m in c(1:nvars)) {
    n <- n+1
    diff
    cv_ga[k,m+1] <- seq(from=vettmin[n],
                          to=vettmax[n],
                          by=(vettmax[n]-vettmin[n])/nvals)[solution[n]]
  }
}
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(
  cv_ga,
  digits = 3,
  caption = "GA precision constraints",
  format = "html"
) |>
  kableExtra::kable_styling(
    bootstrap_options = c("striped", "condensed"),
    full_width = FALSE
  )
```

```{r message=FALSE, warning=FALSE}
ga_solution <- beat.1st(strata,cv_ga)
sum(ga_solution$alloc$ALLOC[-nrow(ga_solution$alloc)])
```

## 3.4 Re-execution of the "pareto" function

With these new precision constraints, the sample size is slightly greater than the affordable. 
So, we apply again the "pareto" function:

```{r message=FALSE, warning=FALSE}
out2 <- pareto(
  strata = strata,
  current_cvs = cv_ga,
  target_size = 50000,
  tolerance = 5,
  cv_caps = caps,
  beat1cv_fun = beat.1cv_2,
  max_iter = 50,
  max_same_iter = 20,
  plot = TRUE,
  plot_dir = "outputs",
  plot_prefix = "domains",
  show_targets = TRUE,     
  show_caps = TRUE      
)
```



# 4. Analysis of the final solution

These is the final set of precision constraints:

```{r message=FALSE, warning=FALSE}
cv_final <- out2$errors_scaled
colnames(cv_final)[2:ncol(cv_final)] <- c(paste0("CV",c(1:nvars)))
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(
  cv_final,
  digits = 3,
  caption = "Final precision constraints",
  format = "html"
) |>
  kableExtra::kable_styling(
    bootstrap_options = c("striped", "condensed"),
    full_width = FALSE
  )
```

that produces the following sample size:

```{r message=FALSE, warning=FALSE}
final_sol <- beat.1st(strata,cv_final)
sum(final_sol$alloc$ALLOC[-nrow(final_sol$alloc)])
```
in line with the affordable sample size.

What about the closeness of the final optimal allocation to the proportional one? 


```{r message=FALSE, warning=FALSE}
df <- pareto_alloc$alloc
df <- df[-nrow(df),]
rng = c(0, max(df$ALLOC,df$PROP))   
p1 <- plot(ALLOC  ~  PROP, 
     data = df,
     main = "Optimal vs Proportional allocation - Before GA adjustment",
     ylab = "Optimal",
     xlab = "Proportional",
     xlim = rng,
     ylim = rng,
     pch = 21,
     bg  = "steelblue", 
     col = "black",
     cex = 1.2)
abline(a = 0, b = 1, col = "red", lwd = 2, lty = 2)
text(
  x = rng[2] * 0.8,
  y = rng[2] * 0.9,
  labels = "y = x",
  col = "red",
  pos = 4,
  cex = 0.9
)
fit <- lm(ALLOC~PROP, data = df)
coefs <- coef(fit)
r2 <- summary(fit)$r.squared
abline(fit, col = "darkgreen", lwd = 2)
text(
  x = rng[2] * 0.75,
  y = rng[2] * 0.5,
  labels = sprintf("y = %.2f + %.2f x\nR² = %.2f",
                   coefs[1], coefs[2], r2),
  col = "darkgreen",
  pos = 4,
  cex = 0.9
)


df <- final_sol$alloc
df <- df[-nrow(df),]
rng = c(0, max(df$ALLOC,df$PROP))   
p2 <- plot(ALLOC  ~  PROP, 
     data = df,
     main = "Optimal vs Proportional allocation - After GA adjustment",
     ylab = "Optimal",
     xlab = "Proportional",
     xlim = rng,
     ylim = rng,
     pch = 21,
     bg  = "steelblue", 
     col = "black",
     cex = 1.2)
abline(a = 0, b = 1, col = "red", lwd = 2, lty = 2)
text(
  x = rng[2] * 0.8,
  y = rng[2] * 0.9,
  labels = "y = x",
  col = "red",
  pos = 4,
  cex = 0.9
)
fit <- lm(ALLOC~PROP, data = df)
coefs <- coef(fit)
r2 <- summary(fit)$r.squared
abline(fit, col = "darkgreen", lwd = 2)
text(
  x = rng[2] * 0.75,
  y = rng[2] * 0.5,
  labels = sprintf("y = %.2f + %.2f x\nR² = %.2f",
                   coefs[1], coefs[2], r2),
  col = "darkgreen",
  pos = 4,
  cex = 0.9
)
```
So, we obtained a final optimal solution that is much more close to the proportional one with respect to the one obtained with the first application of the "pareto" function. 

It is worth noting that the final correlation:

```{r message=FALSE, warning=FALSE}
cor(final_sol$alloc$ALLOC[-nrow(final_sol$alloc)],final_sol$alloc$PROP[-nrow(final_sol$alloc)])
```

is not the one that we indicated to and obtained by the genetic algorithm. This is due to the fact that when re-applying the "pareto" function, we lost some of the correlation. If we want a higher value of it we should get back to the genetic algorithm and set a higher value for the desired correlation.

Finally, we want to compare the different sets of the precision constraints:

- the initial ones (those obtained by a time consuming human interaction);

- those obtained by applying the "pareto" function to the set of equal CVs;

- the final ones, obtained by executing the genetic algorithm to get the resulting optimal allocation closer to the proportional one (and adjusted once again with the "pareto" function.


```{r message=FALSE, warning=FALSE}
plot_domain <- function(dom_label, cv_pareto, cv_ga, cv_final,
                        series_cols = c("red", "blue","yellow")) {
  
  vnames <- names(cv_pareto)
  
  op <- par(no.readonly = TRUE)
  on.exit(par(op), add = TRUE)
  par(mar = c(8, 4, 4, 2) + 0.1)
  
  ymax <- max(cv_pareto, cv_ga, cv_final, na.rm = TRUE) * 1.15
  
  heights <- rbind(Pareto = cv_pareto, GA = cv_ga, Final = cv_final)
  
  bp <- barplot(
    heights,
    beside = TRUE,
    names.arg = vnames,
    las = 2,
    cex.names = 0.8,
    ylim = c(0, ymax),
    main = paste("CV by variable - Domain:", dom_label),
    ylab = "CV",
    col  = series_cols
  )
  
  # bp è una matrice 2 x nvar con le x dei bar; prendiamo i centri del gruppo
  x_centers <- colMeans(bp)
  
  # linea tratteggiata (cv_ga)
  # lines(x_centers, ga_vals, type = "b", lty = 2, pch = 16)
  
  legend(
    "topleft",
    legend = c("Pareto CVs", "GA CVs", "Final CVs"),
    col    = c(series_cols),
    pch    = c(15, 15, 15),
    bty    = "n",
    cex = 0.7
  )
}


doms <- unique(cv_pareto$DOM)
ncols <- ncol(cv_pareto)
vcols <- names(cv_pareto)[2:ncol(cv_pareto)]

for (d in doms) {
  pareto_row     <- cv_pareto[cv_pareto$DOM  == d, vcols, drop = FALSE]
  ga_row <- cv_ga[cv_ga$DOM == d, vcols, drop = FALSE]
  final_row     <- cv_final[cv_final$DOM  == d, vcols, drop = FALSE]
  
  if (nrow(pareto_row) != 1 || nrow(ga_row) != 1 || nrow(final_row) != 1) {
    warning("Dominio ", d, ": righe non univoche nei file. Salto.")
    next
  }
  
  pareto_vals <- as.numeric(pareto_row[1, ])
  ga_vals     <- as.numeric(ga_row[1, ])
  final_vals     <- as.numeric(final_row[1, ])
  
  names(pareto_vals) <- names(ga_vals) <- names(final_vals) <- vcols
  plot_domain(dom_label = d, cv_pareto = pareto_vals, cv_ga = ga_vals, cv_final = final_vals)
}
```
